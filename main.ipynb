{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### importing necessary libraries\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Defining factory class for model creation\n",
    "class BaseModel(nn.Module):\n",
    "    '''\n",
    "    This is the Facotry class of base learner\n",
    "    '''\n",
    "    def __init__(self, input_dim,n_layers, n_hidden_units, ouput_dim):\n",
    "        super(BaseModel, self).__init__()\n",
    "        '''\n",
    "        input_dim: no of input features\n",
    "        n_layers: no of hidden layers for this base learner except the output layer\n",
    "        n_hidden_units: array of size n_layers containing hidden units for each n_layers[i]\n",
    "        ouput_dim: dimension of output units\n",
    "        '''\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(input_dim, n_hidden_units[i]))\n",
    "            layers.append(nn.BatchNorm1d(num_features=n_hidden_units[i]))\n",
    "            layers.append(nn.LeakyReLU(0.1))\n",
    "            input_dim = n_hidden_units[i]\n",
    "\n",
    "        layers.append(nn.Linear(n_hidden_units[-1], ouput_dim))\n",
    "        layers.append(nn.Softmax(1))\n",
    "\n",
    "        self.layers = layers\n",
    "        self.n_layers = n_layers+1\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleClassifier:\n",
    "    \n",
    "    def __init__(self, model_config):\n",
    "\n",
    "        # Device configuration\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        ########### creating base learners\n",
    "        with open(model_config, \"r\") as stream:\n",
    "            try:\n",
    "                self.model_configs = yaml.safe_load(stream)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "                exit()\n",
    "\n",
    "        models = []\n",
    "        for i in range(len(self.model_configs)):\n",
    "            model_cfg = self.model_configs['model_'+str(i)]\n",
    "            model = BaseModel(input_dim=393, n_layers = model_cfg['n_layers'], \n",
    "                        n_hidden_units = model_cfg['n_hidden_units'], ouput_dim = 10)\n",
    "            model.to(self.device)\n",
    "            models.append((model, model_cfg['learning_rate']))\n",
    "        self.models = models\n",
    "        \n",
    "        \n",
    "        \n",
    "    def load_data(self, csv_file):\n",
    "        ########## load train data\n",
    "        train_df = pd.read_csv(csv_file, index_col=False)\n",
    "        # copy the data\n",
    "        self.traindf_scaled = train_df.copy()\n",
    "        y = self.traindf_scaled.pop('Y').to_frame()\n",
    "        X = self.traindf_scaled\n",
    "        # apply normalization techniques\n",
    "        for column in X.columns:\n",
    "            X[column] = (X[column] - X[column].min()) / (X[column].max() - X[column].min())\n",
    "\n",
    "        y = y.to_numpy().reshape(-1)\n",
    "        X = X.to_numpy()\n",
    "        #train_df.groupby('Y').size()\n",
    "        return X, y\n",
    "    \n",
    "    def load_test_data(self, csv_file):\n",
    "        test_df = pd.read_csv(csv_file)\n",
    "        X = self.traindf_scaled\n",
    "        \n",
    "        # copy the data\n",
    "        test_df_scaled = test_df.copy()\n",
    "        y_test = test_df_scaled.pop('Y').to_frame()\n",
    "        X_test = test_df_scaled\n",
    "        # apply normalization techniques\n",
    "        for column in X.columns:\n",
    "            X_test[column] = (X_test[column] - X[column].min()) / (X[column].max() - X[column].min())\n",
    "\n",
    "        X_test, y_test = torch.FloatTensor(X_test.to_numpy()).to(self.device ), y_test.to_numpy()\n",
    "        return X_test, y_test\n",
    "\n",
    "    def train_base_learners(self, epochs, batch_size, X, y):\n",
    "        \n",
    "        ############# training of base learners with stratified k-fold cross validation\n",
    "        skf = StratifiedKFold(n_splits = len(self.model_configs), random_state=42, shuffle = True)\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.writer = SummaryWriter()\n",
    "        \n",
    "        ################ training base learners -\n",
    "        for fold_idx, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
    "\n",
    "            print(\"TRAIN:\", train_index, \"VALID:\", valid_index)\n",
    "            X_train, X_valid = torch.FloatTensor(X[train_index]).to(self.device), torch.FloatTensor(X[valid_index]).to(self.device)\n",
    "            y_train, y_valid = torch.LongTensor(y[train_index]).to(self.device), torch.LongTensor(y[valid_index]).to(self.device)\n",
    "\n",
    "            model,lr = self.models[fold_idx]\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            model_name = list(self.model_configs.keys())[fold_idx]\n",
    "            log = 'started training---{}'.format(model_name)\n",
    "            print(log )\n",
    "\n",
    "            best_acc = 0.0\n",
    "            for epoch in range(epochs):\n",
    "                train_loss, acc = self.training_loop(model, X_train, y_train, optimizer)\n",
    "                self.writer.add_scalar('train/loss/model_{}'.format(str(fold_idx)), train_loss, epoch)\n",
    "                self.writer.add_scalar('train/acc/model_{}'.format(str(fold_idx)), acc, epoch)\n",
    "                log = 'Training epoch:{} loss:{} accuracy:{}'.format(epoch, train_loss, acc)\n",
    "                print(log)\n",
    "\n",
    "                valid_loss, acc = self.validation_loop(model, X_valid, y_valid)\n",
    "                self.writer.add_scalar('valid/loss/model_{}'.format(str(fold_idx)), valid_loss, epoch)\n",
    "                self.writer.add_scalar('valid/acc/model_{}'.format(str(fold_idx)), acc, epoch)\n",
    "                log = 'Validation epoch:{} loss:{} accuracy:{}'.format(epoch, valid_loss, acc)\n",
    "                print(log)\n",
    "                #----------save best valid acc model\n",
    "                if acc > best_acc:\n",
    "                    torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
    "                    best_acc = acc\n",
    "            self.writer.close()\n",
    "\n",
    "    def training_loop(self, model, X_train, y_train, optimizer):\n",
    "        #------------- training ---------------\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        y_pred_all = []\n",
    "        for batch_idx in range(0,len(X_train),self.batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            if batch_idx+self.batch_size < len(X_train):\n",
    "                X_batch_train, y_batch_train = X_train[batch_idx : batch_idx+self.batch_size], y_train[batch_idx : batch_idx+self.batch_size]\n",
    "            else:\n",
    "                X_batch_train, y_batch_train = X_train[batch_idx : ], y_train[batch_idx :]\n",
    "\n",
    "            y_pred = model(X_batch_train)\n",
    "            loss = self.loss_fn(y_pred, y_batch_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_pred_all.extend(y_pred.argmax(1).detach().cpu().numpy())\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        iteration = len(X_train)//self.batch_size\n",
    "        train_loss /= iteration\n",
    "        acc = accuracy_score(y_train.cpu().numpy(), y_pred_all)\n",
    "        return train_loss, acc\n",
    "        \n",
    "        \n",
    "    def validation_loop(self, model, X_valid, y_valid):\n",
    "        #-------validation---------\n",
    "        valid_loss = 0\n",
    "        model.eval()\n",
    "        y_pred_all = []\n",
    "        for batch_idx in range(0,len(X_valid), self.batch_size):\n",
    "            if batch_idx+self.batch_size < len(X_valid):\n",
    "                #print(batch_idx+batch_size)\n",
    "                X_batch_valid, y_batch_valid = X_valid[batch_idx : batch_idx+self.batch_size], y_valid[batch_idx : batch_idx+self.batch_size]\n",
    "            else:\n",
    "                X_batch_valid, y_batch_valid = X_valid[batch_idx : ], y_valid[batch_idx :]\n",
    "\n",
    "            y_pred = model(X_batch_valid)\n",
    "            loss = self.loss_fn(y_pred, y_batch_valid)\n",
    "            valid_loss += loss.item()\n",
    "            y_pred_all.extend(y_pred.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "        iteration = len(X_valid)//self.batch_size\n",
    "        valid_loss /= iteration\n",
    "        \n",
    "        acc = accuracy_score(y_valid.cpu().numpy(), y_pred_all)\n",
    "        return valid_loss, acc\n",
    "        \n",
    "        \n",
    "    def voting(self, batch_size, X_test, y_test):\n",
    "        ############ ensemble voting of base learners\n",
    "        for i, (model,_) in enumerate(self.models):\n",
    "            model_name = list(self.model_configs.keys())[i]\n",
    "            model.load_state_dict(torch.load('{}.pt'.format(model_name)))\n",
    "\n",
    "        model_pred, final_pred = [[] for _ in range(len(self.models))], []\n",
    "        for batch_idx in range(0,len(X_test),batch_size):\n",
    "            data = X_test[batch_idx : batch_idx+batch_size]\n",
    "            labels = y_test[batch_idx : batch_idx+batch_size]\n",
    "            y_pred = np.zeros((len(labels),10), dtype='float')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, (model,_) in enumerate(self.models):\n",
    "                    y_batch_pred = model(data)\n",
    "                    model_pred[i].extend(y_batch_pred.argmax(1).detach().cpu().numpy())\n",
    "                    y_pred += y_batch_pred.detach().cpu().numpy()\n",
    "\n",
    "            y_pred /= len(self.models)\n",
    "            y_pred = np.argmax(y_pred, axis=1) \n",
    "            final_pred.extend(y_pred)\n",
    "\n",
    "        #print(model_pred)\n",
    "        for i in range(len(model_pred)):\n",
    "            acc = accuracy_score(y_test, model_pred[i])\n",
    "            print('model{} accuracy:'.format(str(i)),acc)\n",
    "\n",
    "        acc = accuracy_score(y_test, final_pred)\n",
    "        print('final accuracy: ',acc)\n",
    "        cm = confusion_matrix(y_test, final_pred, labels=np.unique(y_test))\n",
    "        print('confusion matrix:\\n',cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     2 ... 29996 29997 29998] VALID: [    5     6    19 ... 29976 29995 29999]\n",
      "started training---model_0\n",
      "Training epoch:0 loss:1.8966492172876994 accuracy:0.5647916666666667\n",
      "Validation epoch:0 loss:1.7814447777794007 accuracy:0.6896666666666667\n",
      "Training epoch:1 loss:1.7589945891698202 accuracy:0.7026666666666667\n",
      "Validation epoch:1 loss:1.7175252526839149 accuracy:0.7513333333333333\n",
      "Training epoch:2 loss:1.713661168575287 accuracy:0.747625\n",
      "Validation epoch:2 loss:1.6830464021407348 accuracy:0.7878333333333334\n",
      "Training epoch:3 loss:1.6925141867001852 accuracy:0.7682083333333334\n",
      "Validation epoch:3 loss:1.6892018381924552 accuracy:0.781\n",
      "Training epoch:4 loss:1.6741395177841187 accuracy:0.7864583333333334\n",
      "Validation epoch:4 loss:1.6550396387589807 accuracy:0.8126666666666666\n",
      "Training epoch:5 loss:1.6631477071444194 accuracy:0.7973333333333333\n",
      "Validation epoch:5 loss:1.6486811300012516 accuracy:0.8205\n",
      "Training epoch:6 loss:1.652392089207967 accuracy:0.8080833333333334\n",
      "Validation epoch:6 loss:1.6480968419243307 accuracy:0.8205\n",
      "Training epoch:7 loss:1.6444595184326172 accuracy:0.8159166666666666\n",
      "Validation epoch:7 loss:1.6494993959518678 accuracy:0.8205\n",
      "Training epoch:8 loss:1.6394123187065124 accuracy:0.8211666666666667\n",
      "Validation epoch:8 loss:1.6347467682578347 accuracy:0.8341666666666666\n",
      "Training epoch:9 loss:1.6282082433700562 accuracy:0.8327083333333334\n",
      "Validation epoch:9 loss:1.6429945816968214 accuracy:0.826\n",
      "Training epoch:10 loss:1.627303354581197 accuracy:0.8330416666666667\n",
      "Validation epoch:10 loss:1.6383549720845758 accuracy:0.8291666666666667\n",
      "Training epoch:11 loss:1.6198887861569722 accuracy:0.8409583333333334\n",
      "Validation epoch:11 loss:1.6286012348644237 accuracy:0.8411666666666666\n",
      "Training epoch:12 loss:1.6131191782951355 accuracy:0.8479583333333334\n",
      "Validation epoch:12 loss:1.6212191301233627 accuracy:0.847\n",
      "Training epoch:13 loss:1.6084915583928425 accuracy:0.8525416666666666\n",
      "Validation epoch:13 loss:1.62406940574952 accuracy:0.845\n",
      "Training epoch:14 loss:1.6089022348721822 accuracy:0.8515\n",
      "Validation epoch:14 loss:1.6157679666172375 accuracy:0.8521666666666666\n",
      "Training epoch:15 loss:1.6057023951212566 accuracy:0.85475\n",
      "Validation epoch:15 loss:1.6134723781901885 accuracy:0.8565\n",
      "Training epoch:16 loss:1.604028885046641 accuracy:0.8569166666666667\n",
      "Validation epoch:16 loss:1.621981443568347 accuracy:0.8481666666666666\n",
      "Training epoch:17 loss:1.5996814047495524 accuracy:0.8610833333333333\n",
      "Validation epoch:17 loss:1.6074363388479713 accuracy:0.8601666666666666\n",
      "Training epoch:18 loss:1.5987734551429749 accuracy:0.8620833333333333\n",
      "Validation epoch:18 loss:1.6062380362321986 accuracy:0.8628333333333333\n",
      "Training epoch:19 loss:1.5906770437558493 accuracy:0.8702916666666667\n",
      "Validation epoch:19 loss:1.6109182790001446 accuracy:0.8576666666666667\n",
      "Training epoch:20 loss:1.5889306151072184 accuracy:0.8714166666666666\n",
      "Validation epoch:20 loss:1.615240834613535 accuracy:0.8528333333333333\n",
      "Training epoch:21 loss:1.5870201579729717 accuracy:0.873375\n",
      "Validation epoch:21 loss:1.6004500714215366 accuracy:0.8688333333333333\n",
      "Training epoch:22 loss:1.584666172504425 accuracy:0.8766666666666667\n",
      "Validation epoch:22 loss:1.61092756839997 accuracy:0.8576666666666667\n",
      "Training epoch:23 loss:1.5813522726694742 accuracy:0.879375\n",
      "Validation epoch:23 loss:1.6075377572666516 accuracy:0.8628333333333333\n",
      "Training epoch:24 loss:1.5797021045684814 accuracy:0.880625\n",
      "Validation epoch:24 loss:1.611107227636531 accuracy:0.8578333333333333\n",
      "Training epoch:25 loss:1.584390991528829 accuracy:0.8764583333333333\n",
      "Validation epoch:25 loss:1.6138479123141038 accuracy:0.8558333333333333\n",
      "Training epoch:26 loss:1.579869119644165 accuracy:0.8810416666666666\n",
      "Validation epoch:26 loss:1.601233860388159 accuracy:0.8683333333333333\n",
      "Training epoch:27 loss:1.578356193224589 accuracy:0.8823333333333333\n",
      "Validation epoch:27 loss:1.6043951683503421 accuracy:0.8653333333333333\n"
     ]
    }
   ],
   "source": [
    "########## driver code\n",
    "ensembleClf = EnsembleClassifier('model_config.yaml')\n",
    "X, y = ensembleClf.load_data('train.csv')\n",
    "ensembleClf.train_base_learners(epochs=50, batch_size=32, X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3874f66c58ff4c14\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3874f66c58ff4c14\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir runs --port=6008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model0 accuracy: 0.7075\n",
      "model1 accuracy: 0.6985\n",
      "model2 accuracy: 0.7298\n",
      "model3 accuracy: 0.7495\n",
      "model4 accuracy: 0.7174\n",
      "final accuracy:  0.761\n",
      "confusion matrix:\n",
      " [[ 897    0   15    9    0    1   14    3   12   12]\n",
      " [   1 1067   18    4   26    3    0    6   13    2]\n",
      " [  33   12  682   26   82    3   23   71   57    6]\n",
      " [   9    3   29  760    7   29   41   14  101   15]\n",
      " [   5   12   76    2  691   39   22   72   11   53]\n",
      " [  16    4    6   39   33  680   42   19   32   24]\n",
      " [  21    5    7   25   17   45  697   53   14  116]\n",
      " [   8   14   68    9   82   11   69  715    5   37]\n",
      " [   7   10   44   63   38   28   19   13  752   11]\n",
      " [  27    8   14   12   52   37  123   60   11  669]]\n"
     ]
    }
   ],
   "source": [
    "########### evaluate on test data by soft voting\n",
    "X_test, y_test = ensembleClf.load_test_data('test.csv')\n",
    "ensembleClf.voting(batch_size=32, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
